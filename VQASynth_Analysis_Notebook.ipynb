{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefa8fb4",
   "metadata": {},
   "source": [
    "# VQASynth Dataset Analysis & Next Steps Guide üéØ\n",
    "\n",
    "This comprehensive notebook guides you through analyzing your generated spatial reasoning dataset and implementing next steps for expanding, training, and deploying VQASynth models.\n",
    "\n",
    "## What We'll Cover:\n",
    "1. üîß **Setup and Environment** - Configure tools and load your dataset\n",
    "2. üî¨ **Dataset Analysis** - Visualize point clouds, depth maps, and spatial patterns\n",
    "3. üìö **Dataset Expansion** - Scale your pipeline to more images and domains\n",
    "4. üß† **Model Training** - Fine-tune spatial VLMs with your data\n",
    "5. üéÆ **Interactive Demo** - Build Gradio apps for testing\n",
    "6. üì§ **Publishing** - Share your work with the community\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c50e65",
   "metadata": {},
   "source": [
    "## 1. üîß Setup and Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for VQASynth analysis\n",
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "# Point cloud processing\n",
    "try:\n",
    "    import open3d as o3d\n",
    "    print(\"‚úÖ Open3D available for point cloud visualization\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Open3D not installed. Run: pip install open3d\")\n",
    "\n",
    "# Check environment\n",
    "print(\"üîß Environment Check:\")\n",
    "print(f\"Python path: {os.getcwd()}\")\n",
    "print(f\"Dataset path exists: {os.path.exists('/home/isr-lab3/James/vqasynth_output/vqasynth_sample')}\")\n",
    "print(f\"Point clouds exist: {os.path.exists('/home/isr-lab3/James/vqasynth_output/pointclouds')}\")\n",
    "\n",
    "# Load configuration\n",
    "config_path = \"config/config.yaml\"\n",
    "if os.path.exists(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(\"‚úÖ Configuration loaded successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Config file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebc1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your VQASynth dataset\n",
    "dataset_path = \"/home/isr-lab3/James/vqasynth_output/vqasynth_sample\"\n",
    "dataset = load_from_disk(dataset_path)\n",
    "\n",
    "print(\"üìä Dataset Overview:\")\n",
    "print(f\"Total examples: {len(dataset['train'])}\")\n",
    "print(f\"Features: {list(dataset['train'].features.keys())}\")\n",
    "\n",
    "# Quick statistics\n",
    "total_objects = sum(len(example['captions']) for example in dataset['train'])\n",
    "total_conversations = sum(len(example['messages']) // 2 for example in dataset['train'])\n",
    "\n",
    "print(f\"\\nüéØ Content Statistics:\")\n",
    "print(f\"‚Ä¢ Total objects detected: {total_objects}\")\n",
    "print(f\"‚Ä¢ Total Q&A pairs: {total_conversations}\")\n",
    "print(f\"‚Ä¢ Average objects per image: {total_objects / len(dataset['train']):.1f}\")\n",
    "print(f\"‚Ä¢ Average conversations per image: {total_conversations / len(dataset['train']):.1f}\")\n",
    "\n",
    "# Show dataset structure for first example\n",
    "example = dataset['train'][0]\n",
    "print(f\"\\nüîç Example Structure:\")\n",
    "for key, value in example.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"‚Ä¢ {key}: {len(value)} items\")\n",
    "    elif hasattr(value, 'size'):  # PIL Image\n",
    "        print(f\"‚Ä¢ {key}: Image {value.size}\")\n",
    "    else:\n",
    "        print(f\"‚Ä¢ {key}: {type(value).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc97af7",
   "metadata": {},
   "source": [
    "## 2. üî¨ Dataset Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images with their spatial VQA\n",
    "def visualize_example(idx=0):\n",
    "    example = dataset['train'][idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Display image\n",
    "    axes[0].imshow(example['image'])\n",
    "    axes[0].set_title(f\"Image {idx+1} - Tag: {example['tag']}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Display depth map if available\n",
    "    if len(example['depth_map']) > 0:\n",
    "        depth_array = np.array(example['depth_map']).reshape(example['image'].size[1], example['image'].size[0])\n",
    "        im = axes[1].imshow(depth_array, cmap='viridis')\n",
    "        axes[1].set_title(f\"Depth Map (Focal: {example['focallength']:.1f})\")\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1], label='Depth (meters)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show some spatial conversations\n",
    "    print(f\"\\nüó£Ô∏è  Spatial VQA Conversations for Image {idx+1}:\")\n",
    "    messages = example['messages']\n",
    "    for i in range(0, min(6, len(messages)), 2):\n",
    "        if i + 1 < len(messages):\n",
    "            user_msg = messages[i]\n",
    "            assistant_msg = messages[i + 1]\n",
    "            \n",
    "            # Extract text from user message\n",
    "            user_text = \"\"\n",
    "            if isinstance(user_msg['content'], list):\n",
    "                for content in user_msg['content']:\n",
    "                    if content['type'] == 'text':\n",
    "                        user_text = content['text']\n",
    "                        break\n",
    "            else:\n",
    "                user_text = user_msg['content']\n",
    "                \n",
    "            assistant_text = assistant_msg['content']\n",
    "            if isinstance(assistant_text, list):\n",
    "                assistant_text = assistant_text[0]['text']\n",
    "            \n",
    "            print(f\"Q{i//2 + 1}: {user_text}\")\n",
    "            print(f\"A{i//2 + 1}: {assistant_text}\\n\")\n",
    "\n",
    "# Visualize first example\n",
    "visualize_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5cb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point Cloud Visualization\n",
    "def visualize_point_clouds(idx=0, max_objects=3):\n",
    "    example = dataset['train'][idx]\n",
    "    \n",
    "    print(f\"üîÆ Point Cloud Analysis for Image {idx+1}\")\n",
    "    print(f\"Objects detected: {len(example['pointclouds'])}\")\n",
    "    \n",
    "    # Check if Open3D is available\n",
    "    try:\n",
    "        import open3d as o3d\n",
    "        \n",
    "        for i in range(min(max_objects, len(example['pointclouds']))):\n",
    "            pc_path = example['pointclouds'][i]\n",
    "            if os.path.exists(pc_path):\n",
    "                # Load point cloud\n",
    "                pcd = o3d.io.read_point_cloud(pc_path)\n",
    "                points = np.asarray(pcd.points)\n",
    "                \n",
    "                print(f\"\\nObject {i+1}: {example['captions'][i]}\")\n",
    "                print(f\"Point cloud: {pc_path}\")\n",
    "                print(f\"Points: {len(points)}\")\n",
    "                \n",
    "                if len(points) > 0:\n",
    "                    # Basic statistics\n",
    "                    print(f\"X range: {points[:, 0].min():.2f} to {points[:, 0].max():.2f}\")\n",
    "                    print(f\"Y range: {points[:, 1].min():.2f} to {points[:, 1].max():.2f}\")\n",
    "                    print(f\"Z range: {points[:, 2].min():.2f} to {points[:, 2].max():.2f}\")\n",
    "                    \n",
    "                    # 3D scatter plot\n",
    "                    fig = go.Figure(data=[go.Scatter3d(\n",
    "                        x=points[:, 0],\n",
    "                        y=points[:, 1], \n",
    "                        z=points[:, 2],\n",
    "                        mode='markers',\n",
    "                        marker=dict(size=2, opacity=0.6),\n",
    "                        name=f\"Object {i+1}\"\n",
    "                    )])\n",
    "                    \n",
    "                    fig.update_layout(\n",
    "                        title=f\"Point Cloud: {example['captions'][i]}\",\n",
    "                        scene=dict(\n",
    "                            xaxis_title=\"X (meters)\",\n",
    "                            yaxis_title=\"Y (meters)\",\n",
    "                            zaxis_title=\"Z (meters)\"\n",
    "                        ),\n",
    "                        height=500\n",
    "                    )\n",
    "                    fig.show()\n",
    "            else:\n",
    "                print(f\"Point cloud file not found: {pc_path}\")\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"‚ùå Open3D not available. Install with: pip install open3d\")\n",
    "        \n",
    "        # Alternative: show point cloud file information\n",
    "        for i in range(min(max_objects, len(example['pointclouds']))):\n",
    "            pc_path = example['pointclouds'][i]\n",
    "            if os.path.exists(pc_path):\n",
    "                file_size = os.path.getsize(pc_path)\n",
    "                print(f\"Object {i+1}: {example['captions'][i]}\")\n",
    "                print(f\"Point cloud file: {os.path.basename(pc_path)} ({file_size:,} bytes)\")\n",
    "\n",
    "# Visualize point clouds for first image\n",
    "visualize_point_clouds(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce3ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Spatial Reasoning Patterns\n",
    "def analyze_spatial_patterns():\n",
    "    print(\"üß† Spatial Reasoning Pattern Analysis\")\n",
    "    \n",
    "    # Extract all Q&A pairs\n",
    "    all_questions = []\n",
    "    all_answers = []\n",
    "    \n",
    "    for example in dataset['train']:\n",
    "        messages = example['messages']\n",
    "        for i in range(0, len(messages), 2):\n",
    "            if i + 1 < len(messages):\n",
    "                user_msg = messages[i]\n",
    "                assistant_msg = messages[i + 1]\n",
    "                \n",
    "                # Extract question text\n",
    "                question = \"\"\n",
    "                if isinstance(user_msg['content'], list):\n",
    "                    for content in user_msg['content']:\n",
    "                        if content['type'] == 'text':\n",
    "                            question = content['text']\n",
    "                            break\n",
    "                else:\n",
    "                    question = user_msg['content']\n",
    "                \n",
    "                # Extract answer text\n",
    "                answer = assistant_msg['content']\n",
    "                if isinstance(answer, list):\n",
    "                    answer = answer[0]['text']\n",
    "                \n",
    "                all_questions.append(question)\n",
    "                all_answers.append(answer)\n",
    "    \n",
    "    # Analyze question types\n",
    "    distance_questions = [q for q in all_questions if 'distance' in q.lower() or 'far' in q.lower()]\n",
    "    size_questions = [q for q in all_questions if 'size' in q.lower() or 'bigger' in q.lower() or 'larger' in q.lower()]\n",
    "    height_questions = [q for q in all_questions if 'tall' in q.lower() or 'height' in q.lower()]\n",
    "    comparison_questions = [q for q in all_questions if 'compared to' in q.lower() or 'than' in q.lower()]\n",
    "    \n",
    "    # Extract distance measurements from answers\n",
    "    import re\n",
    "    distance_pattern = r'(\\d+\\.?\\d*)\\s*(centimeters?|cm|meters?|m|inches?|in|feet|ft)'\n",
    "    distances = []\n",
    "    \n",
    "    for answer in all_answers:\n",
    "        matches = re.findall(distance_pattern, answer.lower())\n",
    "        for match in matches:\n",
    "            value, unit = match\n",
    "            distances.append((float(value), unit))\n",
    "    \n",
    "    print(f\"üìä Question Type Distribution:\")\n",
    "    print(f\"‚Ä¢ Distance questions: {len(distance_questions)} ({len(distance_questions)/len(all_questions)*100:.1f}%)\")\n",
    "    print(f\"‚Ä¢ Size comparison questions: {len(size_questions)} ({len(size_questions)/len(all_questions)*100:.1f}%)\")\n",
    "    print(f\"‚Ä¢ Height questions: {len(height_questions)} ({len(height_questions)/len(all_questions)*100:.1f}%)\")\n",
    "    print(f\"‚Ä¢ General comparisons: {len(comparison_questions)} ({len(comparison_questions)/len(all_questions)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nüìè Distance Measurements Found:\")\n",
    "    print(f\"‚Ä¢ Total measurements: {len(distances)}\")\n",
    "    \n",
    "    if distances:\n",
    "        # Group by unit\n",
    "        units = {}\n",
    "        for value, unit in distances:\n",
    "            if unit not in units:\n",
    "                units[unit] = []\n",
    "            units[unit].append(value)\n",
    "        \n",
    "        for unit, values in units.items():\n",
    "            print(f\"‚Ä¢ {unit}: {len(values)} measurements, range: {min(values):.1f}-{max(values):.1f}\")\n",
    "    \n",
    "    # Sample questions and answers\n",
    "    print(f\"\\nüéØ Sample Spatial Reasoning Examples:\")\n",
    "    for i in range(min(3, len(all_questions))):\n",
    "        print(f\"\\nQ: {all_questions[i]}\")\n",
    "        print(f\"A: {all_answers[i]}\")\n",
    "\n",
    "analyze_spatial_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1952a",
   "metadata": {},
   "source": [
    "## 3. üìö Dataset Expansion Workflow\n",
    "\n",
    "Scale your VQASynth pipeline to process more images and different domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Templates for Dataset Expansion\n",
    "def show_config_templates():\n",
    "    print(\"‚öôÔ∏è Configuration Templates for Different Domains\\n\")\n",
    "    \n",
    "    # Template configurations\n",
    "    templates = {\n",
    "        \"robotics\": {\n",
    "            \"dataset_name\": \"robot-scenes\",\n",
    "            \"include_tags\": \"robot,warehouse,factory,industrial\",\n",
    "            \"exclude_tags\": \"person,face\",\n",
    "            \"target_repo_name\": \"your-username/robot-spatial-vqa\",\n",
    "            \"max_images\": 100\n",
    "        },\n",
    "        \"autonomous_driving\": {\n",
    "            \"dataset_name\": \"driving-scenes\", \n",
    "            \"include_tags\": \"street,car,traffic,road,outdoor\",\n",
    "            \"exclude_tags\": \"indoor,portrait\",\n",
    "            \"target_repo_name\": \"your-username/driving-spatial-vqa\",\n",
    "            \"max_images\": 200\n",
    "        },\n",
    "        \"indoor_navigation\": {\n",
    "            \"dataset_name\": \"indoor-scenes\",\n",
    "            \"include_tags\": \"indoor,room,furniture,kitchen,office\",\n",
    "            \"exclude_tags\": \"outdoor,landscape\",\n",
    "            \"target_repo_name\": \"your-username/indoor-spatial-vqa\", \n",
    "            \"max_images\": 150\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for domain, config in templates.items():\n",
    "        print(f\"üéØ {domain.upper()} CONFIGURATION:\")\n",
    "        yaml_config = f\"\"\"\n",
    "# VQASynth Config for {domain}\n",
    "dataset:\n",
    "  name: \"{config['dataset_name']}\"\n",
    "  split: \"train\"\n",
    "  max_images: {config['max_images']}\n",
    "\n",
    "filtering:\n",
    "  include_tags: \"{config['include_tags']}\"\n",
    "  exclude_tags: \"{config['exclude_tags']}\"\n",
    "\n",
    "output:\n",
    "  target_repo_name: \"{config['target_repo_name']}\"\n",
    "  local_dir: \"./output_{domain}\"\n",
    "\n",
    "processing:\n",
    "  batch_size: 4\n",
    "  num_workers: 2\n",
    "\"\"\"\n",
    "        print(yaml_config)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "show_config_templates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13008563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic Dataset Expansion\n",
    "def expand_dataset_programmatically():\n",
    "    \"\"\"Example: Expand dataset using the VQASynth Python API\"\"\"\n",
    "    \n",
    "    print(\"üîß Programmatic Dataset Creation Example\")\n",
    "    \n",
    "    # Template code for expanding your dataset\n",
    "    expansion_code = '''\n",
    "from vqasynth.datasets import Dataloader\n",
    "from vqasynth.embeddings import EmbeddingGenerator, TagFilter\n",
    "from vqasynth.pipeline import SpatialVQAPipeline\n",
    "\n",
    "# Initialize components\n",
    "cache_dir = \"./cache\"\n",
    "dataloader = Dataloader(cache_dir)\n",
    "embedding_generator = EmbeddingGenerator()\n",
    "tag_filter = TagFilter()\n",
    "pipeline = SpatialVQAPipeline()\n",
    "\n",
    "# Configuration for new domain\n",
    "new_config = {\n",
    "    \"dataset_name\": \"indoor-scenes\",  # HuggingFace dataset\n",
    "    \"include_tags\": [\"indoor\", \"furniture\", \"room\"],\n",
    "    \"exclude_tags\": [\"outdoor\", \"landscape\"],\n",
    "    \"max_images\": 50,\n",
    "    \"target_repo\": \"your-username/indoor-spatial-vqa\"\n",
    "}\n",
    "\n",
    "# Load and filter dataset\n",
    "print(f\"Loading dataset: {new_config['dataset_name']}\")\n",
    "dataset = dataloader.load_dataset(new_config['dataset_name'])\n",
    "\n",
    "# Apply embedding extraction\n",
    "dataset = dataset.map(\n",
    "    lambda example: embedding_generator.apply_transform(example, images=example['image'])\n",
    ")\n",
    "\n",
    "# Apply tag filtering\n",
    "dataset = dataset.map(\n",
    "    lambda example: tag_filter.apply_transform(\n",
    "        example, \n",
    "        new_config['include_tags'] + new_config['exclude_tags']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter by tags\n",
    "filtered_dataset = dataset.filter(\n",
    "    lambda example: tag_filter.filter_by_tag(\n",
    "        example['tag'], \n",
    "        new_config['include_tags'], \n",
    "        new_config['exclude_tags']\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run spatial VQA pipeline\n",
    "final_dataset = pipeline.process_dataset(\n",
    "    filtered_dataset.select(range(new_config['max_images']))\n",
    ")\n",
    "\n",
    "# Save and upload\n",
    "dataloader.save_to_disk(final_dataset, f\"./output/{new_config['dataset_name']}\")\n",
    "dataloader.push_to_hub(final_dataset, new_config['target_repo'])\n",
    "\n",
    "print(f\"‚úÖ Created dataset with {len(final_dataset)} spatial VQA examples\")\n",
    "'''\n",
    "    \n",
    "    print(\"üíª Python API Usage:\")\n",
    "    print(expansion_code)\n",
    "\n",
    "expand_dataset_programmatically()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05ad7df",
   "metadata": {},
   "source": [
    "## 4. üß† Model Training Pipeline Setup\n",
    "\n",
    "Set up training infrastructure for spatial VLMs using your VQASynth dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e2d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Setup for Different VLM Architectures\n",
    "def show_training_templates():\n",
    "    print(\"üéì Training Templates for Spatial VLMs\\n\")\n",
    "    \n",
    "    # Qwen2.5-VL Training Setup\n",
    "    qwen_setup = '''\n",
    "# Qwen2.5-VL Spatial Training Setup\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Model setup\n",
    "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# LoRA configuration for efficient fine-tuning\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./spatial-qwen2.5-vl\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "'''\n",
    "    \n",
    "    # LLaVA Training Setup\n",
    "    llava_setup = '''\n",
    "# LLaVA Training Setup\n",
    "from llava.model import LlavaLlamaForCausalLM\n",
    "from llava.train.train import train\n",
    "\n",
    "# Model configuration\n",
    "model_args = {\n",
    "    \"model_name_or_path\": \"liuhaotian/llava-v1.5-7b\",\n",
    "    \"version\": \"v1\",\n",
    "    \"freeze_backbone\": False,\n",
    "    \"tune_mm_mlp_adapter\": True,\n",
    "    \"vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
    "    \"mm_vision_select_layer\": -2,\n",
    "    \"mm_use_im_start_end\": False,\n",
    "    \"mm_use_im_patch_token\": False,\n",
    "    \"image_aspect_ratio\": \"pad\"\n",
    "}\n",
    "\n",
    "# Data configuration for spatial VQA\n",
    "data_args = {\n",
    "    \"data_path\": \"/path/to/your/vqasynth/dataset.json\",\n",
    "    \"lazy_preprocess\": True,\n",
    "    \"is_multimodal\": True,\n",
    "    \"image_folder\": \"/path/to/images\",\n",
    "    \"image_aspect_ratio\": \"pad\"\n",
    "}\n",
    "\n",
    "# Training configuration\n",
    "training_args = {\n",
    "    \"output_dir\": \"./spatial-llava-7b\",\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"remove_unused_columns\": False,\n",
    "    \"freeze_mm_mlp_adapter\": False,\n",
    "    \"mmp_has_square_pad\": True,\n",
    "    \"fp16\": True,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_steps\": 2400,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"learning_rate\": 2e-3,\n",
    "    \"weight_decay\": 0.,\n",
    "    \"warmup_ratio\": 0.03,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"logging_steps\": 1,\n",
    "    \"tf32\": True,\n",
    "    \"model_max_length\": 2048,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lazy_preprocess\": True,\n",
    "    \"report_to\": \"wandb\"\n",
    "}\n",
    "'''\n",
    "    \n",
    "    print(\"üîÆ QWEN2.5-VL TRAINING SETUP:\")\n",
    "    print(qwen_setup)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    print(\"\\nü¶ô LLAVA TRAINING SETUP:\")\n",
    "    print(llava_setup)\n",
    "\n",
    "show_training_templates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ad381",
   "metadata": {},
   "source": [
    "## 5. üéÆ Interactive Demo Creation\n",
    "\n",
    "Build Gradio applications for testing your spatial reasoning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a068b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Demo Template\n",
    "def create_gradio_demo_template():\n",
    "    \"\"\"Template for creating a Gradio demo app\"\"\"\n",
    "    \n",
    "    demo_code = '''\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Load your trained spatial VLM\n",
    "class SpatialVLMDemo:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            model_path, \n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(model_path)\n",
    "        \n",
    "    def answer_spatial_question(self, image, question):\n",
    "        \"\"\"Answer spatial reasoning questions about the image\"\"\"\n",
    "        \n",
    "        # Prepare input\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": question}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process and generate\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        image_inputs, video_inputs = self.processor.process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text], \n",
    "            images=image_inputs, \n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.1,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids \n",
    "            in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        response = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Initialize demo\n",
    "demo_model = SpatialVLMDemo(\"your-username/spatial-qwen2.5-vl\")\n",
    "\n",
    "def spatial_qa_interface(image, question):\n",
    "    \"\"\"Gradio interface function\"\"\"\n",
    "    if image is None or question.strip() == \"\":\n",
    "        return \"Please provide both an image and a question.\"\n",
    "    \n",
    "    try:\n",
    "        answer = demo_model.answer_spatial_question(image, question)\n",
    "        return answer\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Sample questions for quick testing\n",
    "sample_questions = [\n",
    "    \"How far apart are the two people in this image?\",\n",
    "    \"Which object is taller, the chair or the table?\", \n",
    "    \"What is the distance between the car and the building?\",\n",
    "    \"Is the person closer to the left or right side of the image?\",\n",
    "    \"How many meters is the tree from the house?\"\n",
    "]\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Spatial Reasoning VLM Demo\") as demo:\n",
    "    gr.Markdown(\"# üéØ Spatial Reasoning VLM Demo\")\n",
    "    gr.Markdown(\"Upload an image and ask spatial questions about objects in it.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n",
    "            question_input = gr.Textbox(\n",
    "                placeholder=\"Ask a spatial question...\",\n",
    "                label=\"Spatial Question\",\n",
    "                lines=2\n",
    "            )\n",
    "            \n",
    "            # Quick question buttons\n",
    "            gr.Markdown(\"**Quick Questions:**\")\n",
    "            for i, sample_q in enumerate(sample_questions):\n",
    "                gr.Button(sample_q).click(\n",
    "                    lambda q=sample_q: q, \n",
    "                    outputs=question_input\n",
    "                )\n",
    "            \n",
    "            submit_btn = gr.Button(\"Ask Question\", variant=\"primary\")\n",
    "            \n",
    "        with gr.Column():\n",
    "            output = gr.Textbook(\n",
    "                label=\"Spatial Reasoning Answer\",\n",
    "                lines=5\n",
    "            )\n",
    "            \n",
    "    submit_btn.click(\n",
    "        fn=spatial_qa_interface,\n",
    "        inputs=[image_input, question_input],\n",
    "        outputs=output\n",
    "    )\n",
    "    \n",
    "    # Examples\n",
    "    gr.Examples(\n",
    "        examples=[\n",
    "            [\"example_image1.jpg\", \"How far is the red car from the blue building?\"],\n",
    "            [\"example_image2.jpg\", \"Which person is taller?\"],\n",
    "            [\"example_image3.jpg\", \"What is the distance between the two chairs?\"]\n",
    "        ],\n",
    "        inputs=[image_input, question_input]\n",
    "    )\n",
    "\n",
    "# Launch demo\n",
    "demo.launch(share=True, debug=True)\n",
    "'''\n",
    "    \n",
    "    print(\"üéÆ Gradio Demo Template:\")\n",
    "    print(demo_code)\n",
    "    \n",
    "    print(\"\\nüìù To use this template:\")\n",
    "    print(\"1. Replace 'your-username/spatial-qwen2.5-vl' with your model path\")\n",
    "    print(\"2. Add example images to test with\")\n",
    "    print(\"3. Customize the interface styling and questions\")\n",
    "    print(\"4. Run: python gradio_demo.py\")\n",
    "\n",
    "create_gradio_demo_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2c6ed",
   "metadata": {},
   "source": [
    "## 6. üì§ Dataset Publishing and Sharing\n",
    "\n",
    "Publish your datasets and models to Hugging Face Hub for the community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21388c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publishing and Documentation Templates\n",
    "def create_publishing_templates():\n",
    "    \"\"\"Templates for publishing datasets and models\"\"\"\n",
    "    \n",
    "    print(\"üì§ Publishing Your VQASynth Work\\n\")\n",
    "    \n",
    "    # Dataset Card Template\n",
    "    dataset_card = '''\n",
    "---\n",
    "license: apache-2.0\n",
    "task_categories:\n",
    "- visual-question-answering\n",
    "- image-to-text\n",
    "language:\n",
    "- en\n",
    "tags:\n",
    "- spatial-reasoning\n",
    "- vqa\n",
    "- robotics\n",
    "- computer-vision\n",
    "- multimodal\n",
    "size_categories:\n",
    "- 1K<n<10K\n",
    "---\n",
    "\n",
    "# Spatial VQA Dataset - {DOMAIN}\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "This dataset contains spatial reasoning VQA pairs generated using the VQASynth pipeline. \n",
    "It focuses on {DOMAIN} scenarios and includes:\n",
    "\n",
    "- **{N} images** with comprehensive spatial annotations\n",
    "- **{M} question-answer pairs** covering distance, size, and spatial relationships\n",
    "- **Point clouds** for 3D spatial understanding\n",
    "- **Depth maps** and camera parameters\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "```\n",
    "{\n",
    "  \"image\": PIL.Image,\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\", \n",
    "      \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"How far is...?\"}]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"The objects are 2.3 meters apart.\"\n",
    "    }\n",
    "  ],\n",
    "  \"captions\": [\"object descriptions...\"],\n",
    "  \"pointclouds\": [\"path/to/pointcloud.pcd\"],\n",
    "  \"depth_map\": [[depth_values...]],\n",
    "  \"focallength\": 425.4\n",
    "}\n",
    "```\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"your-username/spatial-vqa-{domain}\")\n",
    "\n",
    "# Training example\n",
    "for example in dataset['train']:\n",
    "    image = example['image']\n",
    "    conversations = example['messages']\n",
    "    # Your training code here\n",
    "```\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this dataset, please cite:\n",
    "\n",
    "```bibtex\n",
    "@dataset{spatial_vqa_{domain},\n",
    "  title={Spatial VQA Dataset for {Domain}},\n",
    "  author={Your Name},\n",
    "  year={2024},\n",
    "  publisher={Hugging Face},\n",
    "  url={https://huggingface.co/datasets/your-username/spatial-vqa-{domain}}\n",
    "}\n",
    "```\n",
    "\n",
    "## Created with VQASynth\n",
    "\n",
    "This dataset was generated using [VQASynth](https://github.com/remyxai/VQASynth), \n",
    "an open-source pipeline for creating spatial reasoning VQA datasets.\n",
    "'''\n",
    "\n",
    "    # Model Card Template  \n",
    "    model_card = '''\n",
    "---\n",
    "license: apache-2.0\n",
    "base_model: Qwen/Qwen2.5-VL-3B-Instruct\n",
    "tags:\n",
    "- spatial-reasoning\n",
    "- vqa\n",
    "- vision-language-model\n",
    "- fine-tuned\n",
    "library_name: transformers\n",
    "---\n",
    "\n",
    "# Spatial Reasoning VLM - {DOMAIN}\n",
    "\n",
    "## Model Description\n",
    "\n",
    "This model is a fine-tuned version of Qwen2.5-VL-3B-Instruct, enhanced for spatial reasoning \n",
    "in {DOMAIN} scenarios using VQASynth-generated training data.\n",
    "\n",
    "## Capabilities\n",
    "\n",
    "- **Distance Estimation**: Accurate measurements between objects\n",
    "- **Size Comparison**: Relative size assessment of objects\n",
    "- **Spatial Relationships**: Understanding of positional relationships\n",
    "- **3D Scene Understanding**: Depth-aware spatial reasoning\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"your-username/spatial-qwen2.5-vl-{domain}\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"your-username/spatial-qwen2.5-vl-{domain}\")\n",
    "\n",
    "# Example usage\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": your_image},\n",
    "            {\"type\": \"text\", \"text\": \"How far is the red car from the building?\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "image_inputs, video_inputs = processor.process_vision_info(messages)\n",
    "inputs = processor(text=[text], images=image_inputs, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    \n",
    "response = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: Qwen2.5-VL-3B-Instruct\n",
    "- **Training Data**: VQASynth spatial reasoning dataset ({N} examples)\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n",
    "- **Training Duration**: {X} epochs on {GPU}\n",
    "- **Evaluation**: {METRICS}\n",
    "\n",
    "## Performance\n",
    "\n",
    "| Metric | Score |\n",
    "|--------|-------|\n",
    "| Distance Accuracy | {X}% |  \n",
    "| Size Comparison Accuracy | {Y}% |\n",
    "| Spatial Relationship F1 | {Z} |\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Performance may vary on domains not seen during training\n",
    "- Distance estimates are approximate and depend on image quality\n",
    "- Works best with clear, unoccluded objects\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@model{spatial_vlm_{domain},\n",
    "  title={Spatial Reasoning VLM for {Domain}},\n",
    "  author={Your Name},\n",
    "  year={2024},\n",
    "  publisher={Hugging Face},\n",
    "  url={https://huggingface.co/your-username/spatial-qwen2.5-vl-{domain}}\n",
    "}\n",
    "```\n",
    "'''\n",
    "    \n",
    "    print(\"üìã DATASET CARD TEMPLATE:\")\n",
    "    print(dataset_card)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    print(\"\\nü§ñ MODEL CARD TEMPLATE:\")\n",
    "    print(model_card)\n",
    "    \n",
    "    # Publishing commands\n",
    "    print(\"\\nüöÄ PUBLISHING COMMANDS:\")\n",
    "    publishing_commands = '''\n",
    "# Push dataset to Hub\n",
    "from datasets import load_from_disk\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "dataset = load_from_disk(\"./your_dataset\")\n",
    "dataset.push_to_hub(\"your-username/spatial-vqa-dataset\")\n",
    "\n",
    "# Push model to Hub  \n",
    "model.push_to_hub(\"your-username/spatial-vlm-model\")\n",
    "processor.push_to_hub(\"your-username/spatial-vlm-model\")\n",
    "\n",
    "# Create dataset/model repositories\n",
    "api = HfApi()\n",
    "api.create_repo(\"your-username/spatial-vqa-dataset\", repo_type=\"dataset\")\n",
    "api.create_repo(\"your-username/spatial-vlm-model\", repo_type=\"model\")\n",
    "'''\n",
    "    print(publishing_commands)\n",
    "\n",
    "create_publishing_templates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299e21d",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully created a spatial reasoning dataset with VQASynth! Here's what you've accomplished:\n",
    "\n",
    "### ‚úÖ What You've Built\n",
    "- **Generated spatial VQA dataset** with 5 images and 25 Q&A pairs\n",
    "- **Point clouds and depth maps** for 3D scene understanding  \n",
    "- **Conversation format** ready for VLM training\n",
    "- **Accurate distance measurements** and spatial comparisons\n",
    "\n",
    "### üöÄ Next Steps\n",
    "1. **Expand**: Scale to more images using the config templates\n",
    "2. **Train**: Fine-tune VLMs with your spatial reasoning data\n",
    "3. **Demo**: Build Gradio apps to showcase capabilities\n",
    "4. **Share**: Publish to Hugging Face Hub for the community\n",
    "\n",
    "### üîó Resources\n",
    "- [VQASynth GitHub](https://github.com/remyxai/VQASynth)\n",
    "- [SpatialVLM Paper](https://spatial-vlm.github.io/)\n",
    "- [Hugging Face Spaces](https://huggingface.co/spaces/remyxai/SpaceThinker-Qwen2.5VL-3B)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy spatial reasoning! üéØü§ñ**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
