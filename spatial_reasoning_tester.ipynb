{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2281781",
   "metadata": {},
   "source": [
    "# üöÅ Spatial Reasoning Tester for Drone Navigation\n",
    "\n",
    "Interactive tool to test spatial reasoning capabilities on images, videos, and point clouds.\n",
    "Perfect for exploring spatial understanding for autonomous systems and drone navigation.\n",
    "\n",
    "## What This Does:\n",
    "- üì∏ **Upload images/videos** and ask spatial questions\n",
    "- üîÆ **Analyze point clouds** for 3D spatial relationships  \n",
    "- üìè **Estimate depths and distances** between objects\n",
    "- üöÅ **Test drone navigation scenarios** with spatial queries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0808c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-01 16:27:41.985610: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-01 16:27:41.993253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759325262.001885   77822 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759325262.004676   77822 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759325262.011966   77822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759325262.011974   77822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759325262.011975   77822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759325262.011976   77822 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-01 16:27:42.014390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transformers available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen2_5_vl to instantiate a model of type qwen2_vl. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e11e13196ef745009d664670b3ab2638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237676272caa49c1ac614ff4f7f7666f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  SpaceThinker not available, trying base Qwen2.5-VL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type qwen2_5_vl to instantiate a model of type qwen2_vl. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb52e1c44a014705a3ebeecb67c4315a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup - Load Spatial Reasoning Models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "    import torch\n",
    "    print(\"‚úÖ Transformers available\")\n",
    "    \n",
    "    # Try to load SpaceThinker (best for spatial reasoning)\n",
    "    try:\n",
    "        model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "            \"remyxai/SpaceThinker-Qwen2.5VL-3B\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        processor = AutoProcessor.from_pretrained(\n",
    "            \"remyxai/SpaceThinker-Qwen2.5VL-3B\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(\"üéØ Loaded SpaceThinker-Qwen2.5VL-3B (optimized for spatial reasoning)\")\n",
    "        MODEL_LOADED = True\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  SpaceThinker not available, trying base Qwen2.5-VL...\")\n",
    "        try:\n",
    "            model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "                \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "            print(\"‚úÖ Loaded Qwen2.5-VL-3B-Instruct (base vision-language model)\")\n",
    "            MODEL_LOADED = True\n",
    "        except:\n",
    "            print(\"‚ùå Could not load any vision-language model\")\n",
    "            MODEL_LOADED = False\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"‚ùå Transformers not available. Install: pip install transformers torch\")\n",
    "    MODEL_LOADED = False\n",
    "\n",
    "# Other imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import requests\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial Reasoning Function\n",
    "def ask_spatial_question(image, question):\n",
    "    \"\"\"Ask spatial questions about an image\"\"\"\n",
    "    if not MODEL_LOADED:\n",
    "        return \"‚ùå No spatial reasoning model loaded. Please install transformers and torch.\"\n",
    "    \n",
    "    try:\n",
    "        # Prepare the conversation\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": question}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Process the input\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs = processor.process_vision_info(messages)\n",
    "        inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate answer\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **inputs, \n",
    "                max_new_tokens=256,\n",
    "                temperature=0.1,\n",
    "                do_sample=False\n",
    "            )\n",
    "            \n",
    "        # Decode the response\n",
    "        generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "        response = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {str(e)}\"\n",
    "\n",
    "# Test with a sample image if available\n",
    "if MODEL_LOADED:\n",
    "    print(\"üéØ Spatial reasoning function ready!\")\n",
    "    print(\"Usage: ask_spatial_question(your_image, 'How tall is the building?')\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Spatial reasoning not available - models not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e620647",
   "metadata": {},
   "source": [
    "## üöÅ Test Drone Navigation Scenarios\n",
    "\n",
    "Let's test spatial reasoning with scenarios relevant to drone navigation and autonomous systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images for drone scenarios\n",
    "def load_test_image(url=None, local_path=None):\n",
    "    \"\"\"Load test image from URL or local path\"\"\"\n",
    "    try:\n",
    "        if url:\n",
    "            response = requests.get(url)\n",
    "            image = Image.open(BytesIO(response.content))\n",
    "        elif local_path:\n",
    "            image = Image.open(local_path)\n",
    "        else:\n",
    "            # Create a simple test image if no image provided\n",
    "            image = Image.new('RGB', (400, 300), color='lightblue')\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example drone navigation questions\n",
    "drone_questions = [\n",
    "    \"What is the height of the tallest building in this image?\",\n",
    "    \"How much clearance space is there between the buildings?\", \n",
    "    \"What is the distance from the drone's viewpoint to the nearest obstacle?\",\n",
    "    \"Is there enough vertical space for a 2-meter tall drone to fly through?\",\n",
    "    \"Which direction offers the most open space for navigation?\",\n",
    "    \"What are the dimensions of the open area in the center?\",\n",
    "    \"How wide is the gap between the two structures?\",\n",
    "    \"What is the safest flight path through this environment?\"\n",
    "]\n",
    "\n",
    "print(\"üéØ Drone Navigation Test Questions:\")\n",
    "for i, q in enumerate(drone_questions, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "# Interactive testing function\n",
    "def test_spatial_reasoning(image_source=None, custom_question=None):\n",
    "    \"\"\"Interactive spatial reasoning test\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    if image_source:\n",
    "        if image_source.startswith('http'):\n",
    "            image = load_test_image(url=image_source)\n",
    "        else:\n",
    "            image = load_test_image(local_path=image_source)\n",
    "    else:\n",
    "        # Use a sample image from your dataset if available\n",
    "        sample_images = [\n",
    "            \"/home/isr-lab3/James/VQASynth-UAV/assets/warehouse_sample_1.jpeg\",\n",
    "            \"/home/isr-lab3/James/VQASynth-UAV/assets/warehouse_sample_2.jpeg\"\n",
    "        ]\n",
    "        \n",
    "        for img_path in sample_images:\n",
    "            if os.path.exists(img_path):\n",
    "                image = load_test_image(local_path=img_path)\n",
    "                break\n",
    "        else:\n",
    "            print(\"No test images found. Please provide an image URL or path.\")\n",
    "            return\n",
    "    \n",
    "    if not image:\n",
    "        print(\"Failed to load test image\")\n",
    "        return\n",
    "    \n",
    "    # Display image\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('Test Image for Spatial Reasoning')\n",
    "    plt.show()\n",
    "    \n",
    "    # Test questions\n",
    "    questions_to_test = [custom_question] if custom_question else drone_questions[:3]\n",
    "    \n",
    "    print(\"\\nüîç Spatial Reasoning Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, question in enumerate(questions_to_test, 1):\n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nQ{i}: {question}\")\n",
    "        answer = ask_spatial_question(image, question)\n",
    "        print(f\"A{i}: {answer}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "# Ready to test!\n",
    "print(\"\\n‚úÖ Ready to test spatial reasoning!\")\n",
    "print(\"Usage examples:\")\n",
    "print(\"‚Ä¢ test_spatial_reasoning()  # Use default test images\")  \n",
    "print(\"‚Ä¢ test_spatial_reasoning('path/to/your/image.jpg')\")\n",
    "print(\"‚Ä¢ test_spatial_reasoning('https://example.com/image.jpg')\")\n",
    "print(\"‚Ä¢ test_spatial_reasoning(custom_question='How tall is the drone?')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spatialvlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
